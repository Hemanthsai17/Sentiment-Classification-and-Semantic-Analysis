{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-23T03:43:43.922944Z",
     "iopub.status.busy": "2024-01-23T03:43:43.922668Z",
     "iopub.status.idle": "2024-01-23T03:44:14.459646Z",
     "shell.execute_reply": "2024-01-23T03:44:14.458298Z",
     "shell.execute_reply.started": "2024-01-23T03:43:43.922920Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib -q\n",
    "! pip install -q transformers[torch] einops accelerate bitsandbytes peft trl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:14.462397Z",
     "iopub.status.busy": "2024-01-23T03:44:14.462012Z",
     "iopub.status.idle": "2024-01-23T03:44:32.790103Z",
     "shell.execute_reply": "2024-01-23T03:44:32.789160Z",
     "shell.execute_reply.started": "2024-01-23T03:44:14.462344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:32.791671Z",
     "iopub.status.busy": "2024-01-23T03:44:32.791106Z",
     "iopub.status.idle": "2024-01-23T03:44:32.818474Z",
     "shell.execute_reply": "2024-01-23T03:44:32.817613Z",
     "shell.execute_reply.started": "2024-01-23T03:44:32.791643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d403deff8b406bad6c3199f28390cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:39.246576Z",
     "iopub.status.busy": "2024-01-23T03:44:39.246171Z",
     "iopub.status.idle": "2024-01-23T03:44:39.265193Z",
     "shell.execute_reply": "2024-01-23T03:44:39.264390Z",
     "shell.execute_reply.started": "2024-01-23T03:44:39.246541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/finetune/3.txt',\n",
       " '/kaggle/input/finetune/10.txt',\n",
       " '/kaggle/input/finetune/19.txt',\n",
       " '/kaggle/input/finetune/5.txt',\n",
       " '/kaggle/input/finetune/7.txt',\n",
       " '/kaggle/input/finetune/8.txt',\n",
       " '/kaggle/input/finetune/18.txt',\n",
       " '/kaggle/input/finetune/17.txt',\n",
       " '/kaggle/input/finetune/11.txt',\n",
       " '/kaggle/input/finetune/0.txt',\n",
       " '/kaggle/input/finetune/9.txt',\n",
       " '/kaggle/input/finetune/16.txt',\n",
       " '/kaggle/input/finetune/1.txt',\n",
       " '/kaggle/input/finetune/13.txt',\n",
       " '/kaggle/input/finetune/14.txt',\n",
       " '/kaggle/input/finetune/12.txt',\n",
       " '/kaggle/input/finetune/15.txt',\n",
       " '/kaggle/input/finetune/2.txt',\n",
       " '/kaggle/input/finetune/4.txt',\n",
       " '/kaggle/input/finetune/6.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "files=[]\n",
    "\n",
    "for fi in glob.glob(\"/kaggle/input/finetune/*.txt\"):\n",
    "    files.append(fi)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:40.109177Z",
     "iopub.status.busy": "2024-01-23T03:44:40.108827Z",
     "iopub.status.idle": "2024-01-23T03:44:41.289692Z",
     "shell.execute_reply": "2024-01-23T03:44:41.288784Z",
     "shell.execute_reply.started": "2024-01-23T03:44:40.109147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd08fbba10b74b169518439dcfbc04b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-5e20b56abd253dfb/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730ff05b9c994d2abb35fd89e6f9736a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92dd2c7b704407e80d54e183a224f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-5e20b56abd253dfb/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b8f99f5d694b67a2cbb4823da7e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:48.687995Z",
     "iopub.status.busy": "2024-01-23T03:44:48.687285Z",
     "iopub.status.idle": "2024-01-23T03:44:50.102113Z",
     "shell.execute_reply": "2024-01-23T03:44:50.101297Z",
     "shell.execute_reply.started": "2024-01-23T03:44:48.687964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c772b0eaa164374adb29f9a4e713da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5ff127040c41a99f2c336a8afe5b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307860642526446cb72573a80e075a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cfd4e692f94d13bc8693eb7019717b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=\"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:50.104552Z",
     "iopub.status.busy": "2024-01-23T03:44:50.103880Z",
     "iopub.status.idle": "2024-01-23T03:44:50.108666Z",
     "shell.execute_reply": "2024-01-23T03:44:50.107838Z",
     "shell.execute_reply.started": "2024-01-23T03:44:50.104515Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:50.143845Z",
     "iopub.status.busy": "2024-01-23T03:44:50.143408Z",
     "iopub.status.idle": "2024-01-23T03:44:58.051390Z",
     "shell.execute_reply": "2024-01-23T03:44:58.050390Z",
     "shell.execute_reply.started": "2024-01-23T03:44:50.143805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d803f67235264e24a8dbfc60059acb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35957ec4dafd47dead6b80565a77e0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0153c8fcaa1c406590024318259d3557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29310 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"text\"])\n",
    "    \n",
    "def copy_input_ids(example):\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(copy_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:58.053828Z",
     "iopub.status.busy": "2024-01-23T03:44:58.053476Z",
     "iopub.status.idle": "2024-01-23T03:44:58.058905Z",
     "shell.execute_reply": "2024-01-23T03:44:58.057934Z",
     "shell.execute_reply.started": "2024-01-23T03:44:58.053795Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:58.060321Z",
     "iopub.status.busy": "2024-01-23T03:44:58.060045Z",
     "iopub.status.idle": "2024-01-23T03:44:58.106490Z",
     "shell.execute_reply": "2024-01-23T03:44:58.105547Z",
     "shell.execute_reply.started": "2024-01-23T03:44:58.060297Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:58.108824Z",
     "iopub.status.busy": "2024-01-23T03:44:58.108273Z",
     "iopub.status.idle": "2024-01-23T03:44:58.118501Z",
     "shell.execute_reply": "2024-01-23T03:44:58.117575Z",
     "shell.execute_reply.started": "2024-01-23T03:44:58.108790Z"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:44:59.137815Z",
     "iopub.status.busy": "2024-01-23T03:44:59.137133Z",
     "iopub.status.idle": "2024-01-23T03:46:14.064160Z",
     "shell.execute_reply": "2024-01-23T03:46:14.063112Z",
     "shell.execute_reply.started": "2024-01-23T03:44:59.137782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d28fa216964aa59b7b6605ba791c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e3fdf417e8460f8df33b6b4054838e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e45dfb2f9064835a3442b61800823a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8513fcc733f7487dbe063ac2ae3793e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa886cef04e5453eb5cfdbe66af8763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a984846017469eb0ed94e83bfa141b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638353846afc4aa9a2e0af5e189337fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:46:14.066521Z",
     "iopub.status.busy": "2024-01-23T03:46:14.066082Z",
     "iopub.status.idle": "2024-01-23T03:46:14.137166Z",
     "shell.execute_reply": "2024-01-23T03:46:14.136340Z",
     "shell.execute_reply.started": "2024-01-23T03:46:14.066477Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:46:14.138713Z",
     "iopub.status.busy": "2024-01-23T03:46:14.138330Z",
     "iopub.status.idle": "2024-01-23T03:46:15.587590Z",
     "shell.execute_reply": "2024-01-23T03:46:15.586422Z",
     "shell.execute_reply.started": "2024-01-23T03:46:14.138681Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model,lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:46:15.590421Z",
     "iopub.status.busy": "2024-01-23T03:46:15.589812Z",
     "iopub.status.idle": "2024-01-23T03:46:15.619054Z",
     "shell.execute_reply": "2024-01-23T03:46:15.618332Z",
     "shell.execute_reply.started": "2024-01-23T03:46:15.590386Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:46:15.620454Z",
     "iopub.status.busy": "2024-01-23T03:46:15.620105Z",
     "iopub.status.idle": "2024-01-23T03:46:15.778466Z",
     "shell.execute_reply": "2024-01-23T03:46:15.777444Z",
     "shell.execute_reply.started": "2024-01-23T03:46:15.620419Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"llama-2-7b-test1\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.001,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    #load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:46:15.780284Z",
     "iopub.status.busy": "2024-01-23T03:46:15.779959Z",
     "iopub.status.idle": "2024-01-23T03:54:51.985167Z",
     "shell.execute_reply": "2024-01-23T03:54:51.984379Z",
     "shell.execute_reply.started": "2024-01-23T03:46:15.780258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7328' max='7328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7328/7328 08:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7328, training_loss=0.04915032116086202, metrics={'train_runtime': 515.6452, 'train_samples_per_second': 56.841, 'train_steps_per_second': 14.211, 'total_flos': 7.44131237511168e+16, 'train_loss': 0.04915032116086202, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train(\"/kaggle/working/llama-2-7b-test1/checkpoint-7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:55:23.941457Z",
     "iopub.status.busy": "2024-01-23T03:55:23.940574Z",
     "iopub.status.idle": "2024-01-23T03:55:24.086748Z",
     "shell.execute_reply": "2024-01-23T03:55:24.085759Z",
     "shell.execute_reply.started": "2024-01-23T03:55:23.941422Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_trainer.save_model(\"san-Llama-2-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:57:05.950699Z",
     "iopub.status.busy": "2024-01-23T03:57:05.949738Z",
     "iopub.status.idle": "2024-01-23T03:57:11.318513Z",
     "shell.execute_reply": "2024-01-23T03:57:11.317618Z",
     "shell.execute_reply.started": "2024-01-23T03:57:05.950662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:57:35.751865Z",
     "iopub.status.busy": "2024-01-23T03:57:35.751506Z",
     "iopub.status.idle": "2024-01-23T03:57:42.865392Z",
     "shell.execute_reply": "2024-01-23T03:57:42.864411Z",
     "shell.execute_reply.started": "2024-01-23T03:57:35.751839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfc1ab45a08448295e1251e262ec3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model1=AutoModelForCausalLM.from_pretrained(\"/kaggle/working/san-Llama-2-7B\",load_in_4bit=True,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T03:57:45.380496Z",
     "iopub.status.busy": "2024-01-23T03:57:45.380079Z",
     "iopub.status.idle": "2024-01-23T03:57:48.499541Z",
     "shell.execute_reply": "2024-01-23T03:57:48.498649Z",
     "shell.execute_reply.started": "2024-01-23T03:57:45.380463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[/INST]<>\\nYou are a helpful bot who knows sanskrit. Answer or generate sentences from the user input\\nआर्यत्रिस्कन्ध सूत्रम्\\n<>[/INST]\\nResponse:\\nअहोऽपशेषादीवबलं</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"आर्यत्रिस्कन्ध सूत्रम्\"\n",
    "prompt=f\"\"\"[/INST]<>\n",
    "You are a helpful bot who knows sanskrit. Answer or generate sentences from the user input\n",
    "{q}\\n<>[/INST]\n",
    "Response:\n",
    "\"\"\"\n",
    "encoded_prompt = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(model1.device)\n",
    "output_sequences = model1.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=512,\n",
    "    repetition_penalty = 5.0,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "\n",
    ")\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "    generated_sequences.append(text.strip())\n",
    "generated_sequences[0]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4189467,
     "sourceId": 7234726,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
