{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f87e0",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of Excel files to concatenate\n",
    "excel_files = ['raghuvamsha_sarga_01.xlsx','raghuvamsha_sarga_02.xlsx','raghuvamsha_sarga_03.xlsx',\n",
    "               'raghuvamsha_sarga_04.xlsx','raghuvamsha_sarga_05.xlsx','raghuvamsha_sarga_06.xlsx',\n",
    "               'raghuvamsha_sarga_07.xlsx','raghuvamsha_sarga_08.xlsx','raghuvamsha_sarga_09.xlsx',\n",
    "               'raghuvamsha_sarga_10.xlsx','raghuvamsha_sarga_11.xlsx','raghuvamsha_sarga_12.xlsx']\n",
    "\n",
    "# Create an empty DataFrame to store the concatenated data\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each Excel file and concatenate its data to the combined_data DataFrame\n",
    "for file in excel_files:\n",
    "    # Read data from the Excel file\n",
    "    data = pd.read_excel(file)\n",
    "    \n",
    "    # Concatenate data to the combined_data DataFrame\n",
    "    combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "# Write the combined data to a new Excel file\n",
    "combined_data.to_excel('combined_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31644aad",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9da473",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82165db",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'\\n', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'|', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'॥', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'३', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'१', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'२', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'४', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'५', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'६', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'७', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'९', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'०', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'८', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'-', '', str(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ffc92",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r'\\t', '', str(text)))\n",
    "combined_data['Text'] = combined_data['Text'].apply(lambda text: re.sub(r' ', '', str(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4398b00",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "combined_data['Text'][1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f464f29",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "combined_data.to_excel('manipulated_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b16887",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('manipulated_data.xlsx')\n",
    "\n",
    "# Initialize variables to store the row, column, and maximum size\n",
    "max_size = 0\n",
    "max_row = 0\n",
    "max_col = 0\n",
    "\n",
    "# Iterate over all cells in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    for col in df.columns:\n",
    "        cell_content = str(df.at[index, col])\n",
    "        cell_size = len(cell_content)\n",
    "        if cell_size > max_size:\n",
    "            max_size = cell_size\n",
    "            max_row = index\n",
    "            max_col = col\n",
    "\n",
    "# Print the row, column, and maximum size\n",
    "print(f\"The cell with the maximum size is at row {max_row+1}, column {max_col}, with a size of {max_size}.\")\n",
    "print(f\"The content of the cell is: {df.at[max_row, max_col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4669086",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('manipulated_data.xlsx')\n",
    "\n",
    "# Create an empty list to store the sentiments for each instance\n",
    "sentiments = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the classes where the value is 1\n",
    "    instance_classes = [col for col in df.columns[1:] if row[col] == 1]\n",
    "    \n",
    "    # Append the list of classes to the sentiments list\n",
    "    sentiments.append(instance_classes)\n",
    "\n",
    "# Add a new column 'sentiments' to the DataFrame with the lists of classes\n",
    "df['sentiments'] = sentiments\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bb448",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df.to_excel('sentimented_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facc8bc",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('sentimented_data.xlsx')\n",
    "\n",
    "# Find the cell with the maximum number of elements in the 'sentiments' column\n",
    "max_length = 0\n",
    "max_row_index = -1\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the length of the list in the 'sentiments' column\n",
    "    sentiment_length = len(row['sentiments'])\n",
    "    \n",
    "    # Update max_length and max_row_index if the current length is greater\n",
    "    if sentiment_length > max_length:\n",
    "        max_length = sentiment_length\n",
    "        max_row_index = index\n",
    "\n",
    "# Print the row index and the maximum number of elements\n",
    "print(f\"The row with the maximum number of elements in the 'sentiments' column is at index {max_row_index}, with {max_length} elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81ff0f",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('sentimented_data.xlsx')\n",
    "\n",
    "# Create a new column for each instance to store one sentiment\n",
    "for i in range(len(df)):\n",
    "    # Get one sentiment from the list of sentiments for each instance\n",
    "    sentiment = df.at[i, 'sentiments'][0] if len(df.at[i, 'sentiments']) > 0 else None\n",
    "    \n",
    "    # Add the sentiment to a new column for each instance\n",
    "    df.at[i, 'sentiment'] = sentiment\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538e3eb",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!pip install seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8383f",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# To ignore all warnings (not recommended unless you're sure about the consequences)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46b869",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac241f3",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('/kaggle/input/prepro/manipulated_data.xlsx')\n",
    "\n",
    "# Drop the first column ('Instance') as it's not needed for the heatmap\n",
    "df = df.drop(columns=['Text'])\n",
    "\n",
    "# Calculate the correlation matrix (pairwise frequency of occurrences)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Frequency of occurrences between pairs of classes')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3339cc",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "cross_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fda09",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('/kaggle/input/prepro/manipulated_data.xlsx')\n",
    "\n",
    "# Slice the DataFrame to include only the first 33 classes (columns)\n",
    "df = df.iloc[:, :34]  # Assuming the first column is 'Instance'\n",
    "\n",
    "# Drop the first column ('Instance') as it's not needed for the heatmap\n",
    "df = df.drop(columns=['Text'])\n",
    "\n",
    "# Calculate the correlation matrix (pairwise frequency of occurrences)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Frequency of occurrences between pairs of classes (first 33 classes)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe7966",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel('/kaggle/input/prepro/manipulated_data.xlsx')\n",
    "\n",
    "# Slice the DataFrame to include only the first 33 classes (columns)\n",
    "df = df.iloc[:, :34]  # Assuming the first column is 'Instance'\n",
    "\n",
    "# Drop the first column ('Instance') as it's not needed for the heatmap\n",
    "df = df.drop(columns=['Text'])\n",
    "df = df.drop(columns=['vyadhi - disease (sickness)'])\n",
    "df = df.drop(columns=['apasmara - forgetfulness (epilepsy/dementedness)'])\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix (pairwise frequency of occurrences)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Frequency of occurrences between pairs of classes (first 33 classes)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e31f45",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872c82f",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('/kaggle/input/prepro/manipulated_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca2f2d",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535633f",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69070553",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28244450",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:54:09.649052Z",
     "iopub.status.busy": "2024-03-04T10:54:09.648575Z",
     "iopub.status.idle": "2024-03-04T10:54:12.003958Z",
     "shell.execute_reply": "2024-03-04T10:54:12.002483Z",
     "shell.execute_reply.started": "2024-03-04T10:54:09.649018Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHemanth-Sai/Sentiments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1664\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m ignore_verifications \u001b[38;5;241m=\u001b[39m ignore_verifications \u001b[38;5;129;01mor\u001b[39;00m save_infos\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1664\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1490\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1489\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1490\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1242\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1238\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1242\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1230\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1216\u001b[0m                 path,\n\u001b[1;32m   1217\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1221\u001b[0m             )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1230\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:846\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token\n\u001b[1;32m    837\u001b[0m hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    839\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    840\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    843\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    844\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mget_patterns_in_dataset_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhfh_dataset_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    848\u001b[0m data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    849\u001b[0m     patterns,\n\u001b[1;32m    850\u001b[0m     dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    851\u001b[0m     allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    852\u001b[0m )\n\u001b[1;32m    853\u001b[0m infered_module_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    854\u001b[0m     key: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    856\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:471\u001b[0m, in \u001b[0;36mget_patterns_in_dataset_repository\u001b[0;34m(dataset_info)\u001b[0m\n\u001b[1;32m    469\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_data_files_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:99\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m---> 99\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m \u001b[43mpattern_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    101\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:303\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, allowed_extensions)\u001b[0m\n\u001b[1;32m    301\u001b[0m data_files_ignore \u001b[38;5;241m=\u001b[39m FILES_TO_IGNORE\n\u001b[1;32m    302\u001b[0m fs \u001b[38;5;241m=\u001b[39m HfFileSystem(repo_info\u001b[38;5;241m=\u001b[39mdataset_info)\n\u001b[0;32m--> 303\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPurePath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[1;32m    304\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    305\u001b[0m     filepath\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_files_ignore \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m ]\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:606\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    604\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 606\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43mglob_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mends_with_sep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[1;32m    609\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    610\u001b[0m     p: info\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/utils.py:734\u001b[0m, in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[1;32m    738\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Hemanth-Sai/Sentiments\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22697f72",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2024-03-04T10:54:12.004648Z",
     "iopub.status.idle": "2024-03-04T10:54:12.005016Z",
     "shell.execute_reply": "2024-03-04T10:54:12.004852Z",
     "shell.execute_reply.started": "2024-03-04T10:54:12.004832Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288e905",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2024-03-04T10:54:12.006806Z",
     "iopub.status.idle": "2024-03-04T10:54:12.007371Z",
     "shell.execute_reply": "2024-03-04T10:54:12.007152Z",
     "shell.execute_reply.started": "2024-03-04T10:54:12.007132Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=dataset.train_test_split(train_size=0.8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bf7db7",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:54:12.157060Z",
     "iopub.status.busy": "2024-03-04T10:54:12.156770Z",
     "iopub.status.idle": "2024-03-04T10:54:12.186708Z",
     "shell.execute_reply": "2024-03-04T10:54:12.185552Z",
     "shell.execute_reply.started": "2024-03-04T10:54:12.157037Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800fd0d2",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:54:13.127296Z",
     "iopub.status.busy": "2024-03-04T10:54:13.126611Z",
     "iopub.status.idle": "2024-03-04T10:54:13.159431Z",
     "shell.execute_reply": "2024-03-04T10:54:13.158252Z",
     "shell.execute_reply.started": "2024-03-04T10:54:13.127262Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_val\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_val=dataset['test'].train_test_split(train_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d3b7cf",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T09:42:51.110136Z",
     "iopub.status.busy": "2024-03-04T09:42:51.109833Z",
     "iopub.status.idle": "2024-03-04T09:42:51.116305Z",
     "shell.execute_reply": "2024-03-04T09:42:51.115263Z",
     "shell.execute_reply.started": "2024-03-04T09:42:51.110112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'nirveda - weeping, sighing,indifference,dicouragement', 'glani - guilty', 'sanka - doubt (apprehension)', 'asuya/irsya - jealousy (envy)', 'mada - madness (intoxication)', 'srama - fatigue', 'alasya/alasata - laziness,sitting idle (unwililng to work)', 'dainya - meekness (depression),(despair)', 'cinta - contemplation (anxiety/reflection)', 'moha - bewilderment,[a feeling of being perplexed and confused] (distraction)', 'smrti - rememberance (recollection)', 'dhriti - forbearance,indifference abstenance (equanimity)', 'vrida - shame', 'capalya/capalatha/capala - impudence [rude behavior that does not show respect for others] (unsteadiness)', 'harsa - jubiliation,enjoyment (joy)', 'avega - intense emotion (agitation/flurry)', 'jadya/jadatha - invalidity,looking with steadfast gaze,unable to think properly', 'garva - pride', 'visada - moroseness, sad [quality of being unhappy, annoyed, and unwilling to speak or smile]', 'autsukya - eagerness (impatience/longing)', 'nidra - sleep (drowsiness)', 'apasmara - forgetfulness (epilepsy/dementedness)', 'supti/supta - deep sleep (dreaming)', 'prabodha/vibodha - awakening', 'amarsa - impatience of opposition', 'avahittha - concealment (hiding of true feelings)', 'augrya/ugrata - violence,battle (cruelity/sterness)', 'mati - attention,instructing pupils (resolve)', 'vyadhi - disease (sickness)', 'unmada - craziness (insanity/madness)', 'mriti/marana - death', 'trasa - shock,fear (fright/alarm)', 'vitarka - argument (doubt)', 'utsuka - restless/anxious', 'tarka -deliberation [long and careful consideration or discussion]', 'rati - romantic', 'lajja - shy', 'marsa - patience', 'tyaga - sacrifice', 'vimochana - releif', 'utsaha - hyped/enthused', 'shraddhaadaya - confidence,trust', 'krodha - anger', 'karuna - pity,kind', 'veera - royality,valour,greatness', 'shanta - serene,peaceful,pleasant', 'vismaya - exaggeration/wonder/surprise/pride/doubt', 'bhakti - devotion', 'no emotion'],\n",
       "        num_rows: 103\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'nirveda - weeping, sighing,indifference,dicouragement', 'glani - guilty', 'sanka - doubt (apprehension)', 'asuya/irsya - jealousy (envy)', 'mada - madness (intoxication)', 'srama - fatigue', 'alasya/alasata - laziness,sitting idle (unwililng to work)', 'dainya - meekness (depression),(despair)', 'cinta - contemplation (anxiety/reflection)', 'moha - bewilderment,[a feeling of being perplexed and confused] (distraction)', 'smrti - rememberance (recollection)', 'dhriti - forbearance,indifference abstenance (equanimity)', 'vrida - shame', 'capalya/capalatha/capala - impudence [rude behavior that does not show respect for others] (unsteadiness)', 'harsa - jubiliation,enjoyment (joy)', 'avega - intense emotion (agitation/flurry)', 'jadya/jadatha - invalidity,looking with steadfast gaze,unable to think properly', 'garva - pride', 'visada - moroseness, sad [quality of being unhappy, annoyed, and unwilling to speak or smile]', 'autsukya - eagerness (impatience/longing)', 'nidra - sleep (drowsiness)', 'apasmara - forgetfulness (epilepsy/dementedness)', 'supti/supta - deep sleep (dreaming)', 'prabodha/vibodha - awakening', 'amarsa - impatience of opposition', 'avahittha - concealment (hiding of true feelings)', 'augrya/ugrata - violence,battle (cruelity/sterness)', 'mati - attention,instructing pupils (resolve)', 'vyadhi - disease (sickness)', 'unmada - craziness (insanity/madness)', 'mriti/marana - death', 'trasa - shock,fear (fright/alarm)', 'vitarka - argument (doubt)', 'utsuka - restless/anxious', 'tarka -deliberation [long and careful consideration or discussion]', 'rati - romantic', 'lajja - shy', 'marsa - patience', 'tyaga - sacrifice', 'vimochana - releif', 'utsaha - hyped/enthused', 'shraddhaadaya - confidence,trust', 'krodha - anger', 'karuna - pity,kind', 'veera - royality,valour,greatness', 'shanta - serene,peaceful,pleasant', 'vismaya - exaggeration/wonder/surprise/pride/doubt', 'bhakti - devotion', 'no emotion'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "930ca830",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T09:43:11.199845Z",
     "iopub.status.busy": "2024-03-04T09:43:11.198957Z",
     "iopub.status.idle": "2024-03-04T09:43:11.208863Z",
     "shell.execute_reply": "2024-03-04T09:43:11.207867Z",
     "shell.execute_reply.started": "2024-03-04T09:43:11.199814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'वागर्थाविवसंपृक्तौवागर्थप्रतिपत्तये।जगतःपितरौवन्देपार्वतीपरमेश्वरौ',\n",
       " 'nirveda - weeping, sighing,indifference,dicouragement': 0,\n",
       " 'glani - guilty': 0,\n",
       " 'sanka - doubt (apprehension)': 0,\n",
       " 'asuya/irsya - jealousy (envy)': 0,\n",
       " 'mada - madness (intoxication)': 0,\n",
       " 'srama - fatigue': 0,\n",
       " 'alasya/alasata - laziness,sitting idle (unwililng to work)': 0,\n",
       " 'dainya - meekness (depression),(despair)': 0,\n",
       " 'cinta - contemplation (anxiety/reflection)': 0,\n",
       " 'moha - bewilderment,[a feeling of being perplexed and confused] (distraction)': 0,\n",
       " 'smrti - rememberance (recollection)': 0,\n",
       " 'dhriti - forbearance,indifference abstenance (equanimity)': 0,\n",
       " 'vrida - shame': 0,\n",
       " 'capalya/capalatha/capala - impudence [rude behavior that does not show respect for others] (unsteadiness)': 0,\n",
       " 'harsa - jubiliation,enjoyment (joy)': 0,\n",
       " 'avega - intense emotion (agitation/flurry)': 0,\n",
       " 'jadya/jadatha - invalidity,looking with steadfast gaze,unable to think properly': 0,\n",
       " 'garva - pride': 0,\n",
       " 'visada - moroseness, sad [quality of being unhappy, annoyed, and unwilling to speak or smile]': 0,\n",
       " 'autsukya - eagerness (impatience/longing)': 0,\n",
       " 'nidra - sleep (drowsiness)': 0,\n",
       " 'apasmara - forgetfulness (epilepsy/dementedness)': 0,\n",
       " 'supti/supta - deep sleep (dreaming)': 0,\n",
       " 'prabodha/vibodha - awakening': 0,\n",
       " 'amarsa - impatience of opposition': 0,\n",
       " 'avahittha - concealment (hiding of true feelings)': 0,\n",
       " 'augrya/ugrata - violence,battle (cruelity/sterness)': 0,\n",
       " 'mati - attention,instructing pupils (resolve)': 0,\n",
       " 'vyadhi - disease (sickness)': 0,\n",
       " 'unmada - craziness (insanity/madness)': 0,\n",
       " 'mriti/marana - death': 0,\n",
       " 'trasa - shock,fear (fright/alarm)': 0,\n",
       " 'vitarka - argument (doubt)': 0,\n",
       " 'utsuka - restless/anxious': 0,\n",
       " 'tarka -deliberation [long and careful consideration or discussion]': 0,\n",
       " 'rati - romantic': 0,\n",
       " 'lajja - shy': 0,\n",
       " 'marsa - patience': 0,\n",
       " 'tyaga - sacrifice': 0,\n",
       " 'vimochana - releif': 0,\n",
       " 'utsaha - hyped/enthused': 0,\n",
       " 'shraddhaadaya - confidence,trust': 0,\n",
       " 'krodha - anger': 0,\n",
       " 'karuna - pity,kind': 0,\n",
       " 'veera - royality,valour,greatness': 0,\n",
       " 'shanta - serene,peaceful,pleasant': 0,\n",
       " 'vismaya - exaggeration/wonder/surprise/pride/doubt': 0,\n",
       " 'bhakti - devotion': 1,\n",
       " 'no emotion': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddb0767d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:01:48.407767Z",
     "iopub.status.busy": "2024-03-04T10:01:48.407414Z",
     "iopub.status.idle": "2024-03-04T10:02:01.987758Z",
     "shell.execute_reply": "2024-03-04T10:02:01.986434Z",
     "shell.execute_reply.started": "2024-03-04T10:01:48.407743Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 10:01:53.611922: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 10:01:53.612040: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 10:01:53.739547: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForMaskedLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampathlonka/San-BERT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForMaskedLM' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "model = AutoModelForMaskedLM.from_pretrained('sampathlonka/San-BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d06ea402",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:04:46.613713Z",
     "iopub.status.busy": "2024-03-04T10:04:46.612729Z",
     "iopub.status.idle": "2024-03-04T10:04:46.617887Z",
     "shell.execute_reply": "2024-03-04T10:04:46.617002Z",
     "shell.execute_reply.started": "2024-03-04T10:04:46.613675Z"
    }
   },
   "outputs": [],
   "source": [
    "num_labels=49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03f95d6c",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:04:48.709937Z",
     "iopub.status.busy": "2024-03-04T10:04:48.709016Z",
     "iopub.status.idle": "2024-03-04T10:04:51.674909Z",
     "shell.execute_reply": "2024-03-04T10:04:51.674099Z",
     "shell.execute_reply.started": "2024-03-04T10:04:48.709902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sampathlonka/San-BERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f5e825f8434265971c5aec7aa89a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b42eb981b2490ab97cd7ea83fdee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d45a36bc54a94af6c64dc03dad21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/951k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ca8ce7aaa2469eadba71aa18e2d35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('sampathlonka/San-BERT', num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained('sampathlonka/San-BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1785853",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:06:27.547577Z",
     "iopub.status.busy": "2024-03-04T10:06:27.547186Z",
     "iopub.status.idle": "2024-03-04T10:06:27.760320Z",
     "shell.execute_reply": "2024-03-04T10:06:27.759318Z",
     "shell.execute_reply.started": "2024-03-04T10:06:27.547545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348808a7466d4352baf899f2e1f8e2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0299d6e40fb4bf69022bef234b72df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize input data\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"Text\"], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(tokenize_batch, batched=True)\n",
    "eval_dataset = dataset_val[\"test\"].map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c11ae9",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:06:38.632193Z",
     "iopub.status.busy": "2024-03-04T10:06:38.631774Z",
     "iopub.status.idle": "2024-03-04T10:06:38.638499Z",
     "shell.execute_reply": "2024-03-04T10:06:38.637511Z",
     "shell.execute_reply.started": "2024-03-04T10:06:38.632160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'nirveda - weeping, sighing,indifference,dicouragement', 'glani - guilty', 'sanka - doubt (apprehension)', 'asuya/irsya - jealousy (envy)', 'mada - madness (intoxication)', 'srama - fatigue', 'alasya/alasata - laziness,sitting idle (unwililng to work)', 'dainya - meekness (depression),(despair)', 'cinta - contemplation (anxiety/reflection)', 'moha - bewilderment,[a feeling of being perplexed and confused] (distraction)', 'smrti - rememberance (recollection)', 'dhriti - forbearance,indifference abstenance (equanimity)', 'vrida - shame', 'capalya/capalatha/capala - impudence [rude behavior that does not show respect for others] (unsteadiness)', 'harsa - jubiliation,enjoyment (joy)', 'avega - intense emotion (agitation/flurry)', 'jadya/jadatha - invalidity,looking with steadfast gaze,unable to think properly', 'garva - pride', 'visada - moroseness, sad [quality of being unhappy, annoyed, and unwilling to speak or smile]', 'autsukya - eagerness (impatience/longing)', 'nidra - sleep (drowsiness)', 'apasmara - forgetfulness (epilepsy/dementedness)', 'supti/supta - deep sleep (dreaming)', 'prabodha/vibodha - awakening', 'amarsa - impatience of opposition', 'avahittha - concealment (hiding of true feelings)', 'augrya/ugrata - violence,battle (cruelity/sterness)', 'mati - attention,instructing pupils (resolve)', 'vyadhi - disease (sickness)', 'unmada - craziness (insanity/madness)', 'mriti/marana - death', 'trasa - shock,fear (fright/alarm)', 'vitarka - argument (doubt)', 'utsuka - restless/anxious', 'tarka -deliberation [long and careful consideration or discussion]', 'rati - romantic', 'lajja - shy', 'marsa - patience', 'tyaga - sacrifice', 'vimochana - releif', 'utsaha - hyped/enthused', 'shraddhaadaya - confidence,trust', 'krodha - anger', 'karuna - pity,kind', 'veera - royality,valour,greatness', 'shanta - serene,peaceful,pleasant', 'vismaya - exaggeration/wonder/surprise/pride/doubt', 'bhakti - devotion', 'no emotion', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 824\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92f05c1",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:52:26.517145Z",
     "iopub.status.busy": "2024-03-04T10:52:26.516720Z",
     "iopub.status.idle": "2024-03-04T10:52:33.355995Z",
     "shell.execute_reply": "2024-03-04T10:52:33.355125Z",
     "shell.execute_reply.started": "2024-03-04T10:52:26.517110Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38d8182",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-03-04T10:53:52.486685Z",
     "iopub.status.busy": "2024-03-04T10:53:52.486190Z",
     "iopub.status.idle": "2024-03-04T10:53:52.613765Z",
     "shell.execute_reply": "2024-03-04T10:53:52.612662Z",
     "shell.execute_reply.started": "2024-03-04T10:53:52.486657Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrail\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Define Trainer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     18\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Trail\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b445f4",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49476092",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4535494,
     "sourceId": 7756334,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
